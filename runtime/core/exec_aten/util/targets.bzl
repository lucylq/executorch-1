load("@fbsource//xplat/executorch/build:runtime_wrapper.bzl", "runtime")

def define_common_targets():
    """Defines targets that should be shared between fbcode and xplat.

    The directory containing this targets.bzl file should also contain both
    TARGETS and BUCK files that call this function.
    """

    runtime.genrule(
        name = "selected_mobile_ops_gen",
        macros_only = False,
        cmd = ("echo '{header}' > $OUT/selected_mobile_ops.h").format(
            header =
                """
#pragma once
/**
 * Generated by executorch/codegen/tools/gen_selected_mobile_ops_header.py
 */
inline constexpr bool should_include_kernel_dtype(
  const char *operator_name,
  exec_aten::ScalarType scalar_type
) {
  return (!string_view(operator_name).compare("test_op")
        && (scalar_type == exec_aten::ScalarType::Float || scalar_type == exec_aten::ScalarType::Int));
}
        """,
        ),
        outs = {"selected_mobile_ops": ["selected_mobile_ops.h"]},
        default_outs = ["."],
        visibility = [
            "//executorch/...",
            "@EXECUTORCH_CLIENTS",
        ],
    )

    runtime.cxx_library(
        name = "scalar_type_util_TEST_ONLY",
        srcs = [],
        exported_headers = [
            "scalar_type_util.h",
            ":selected_mobile_ops_gen[selected_mobile_ops]",
        ],
        visibility = [
            "//executorch/...",
            "@EXECUTORCH_CLIENTS",
        ],
        exported_preprocessor_flags = ["-DEXECUTORCH_SELECTIVE_BUILD_DTYPE"],
        exported_deps = ["//executorch/runtime/core/portable_type:scalar_type"],
    )

    for aten_mode in (True, False):
        aten_suffix = "_aten" if aten_mode else ""

        runtime.cxx_library(
            name = "scalar_type_util" + aten_suffix,
            srcs = [],
            exported_headers = [
                "scalar_type_util.h",
            ],
            visibility = [
                "//executorch/...",
                "@EXECUTORCH_CLIENTS",
            ],
            exported_preprocessor_flags = ["-DUSE_ATEN_LIB"] if aten_mode else [],
            exported_deps = [
                "//executorch/runtime/core:core",
            ] + [] if aten_mode else ["//executorch/runtime/core/portable_type:scalar_type"],
            exported_external_deps = ["libtorch"] if aten_mode else [],
        )

        runtime.cxx_library(
            name = "dim_order_util" + aten_suffix,
            srcs = [],
            exported_headers = [
                "dim_order_util.h",
            ],
            exported_deps = [
                "//executorch/runtime/core:core",
            ],
            visibility = [
                "//executorch/...",
                "@EXECUTORCH_CLIENTS",
            ],
            exported_preprocessor_flags = ["-DUSE_ATEN_LIB"] if aten_mode else [],
        )

        runtime.cxx_library(
            name = "tensor_util" + aten_suffix,
            srcs = ["tensor_util_aten.cpp"] if aten_mode else ["tensor_util_portable.cpp"],
            exported_headers = [
                "tensor_util.h",
            ],
            visibility = [
                "//executorch/...",
                "@EXECUTORCH_CLIENTS",
            ],
            exported_preprocessor_flags = ["-DUSE_ATEN_LIB"] if aten_mode else [],
            exported_deps = [
                "//executorch/runtime/core:core",
            ] + [
                "//executorch/runtime/core/exec_aten:lib" + aten_suffix,
                ":scalar_type_util" + aten_suffix,
                ":dim_order_util" + aten_suffix,
            ],
            # WARNING: using a deprecated API to avoid being built into a shared
            # library. In the case of dynamically loading so library we don't want
            # it to depend on other so libraries because that way we have to
            # specify library directory path.
            force_static = True,
        )
